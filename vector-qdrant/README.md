# Qdrant 1m points test setup

## üéØ Goal of the repository
This project is about testing [Qdrant Vector Database](https://qdrant.tech/) to handle 1 million points using different query strategies.

It consists of three components:

1. Docker compose that sets up:
    - A three node Qdrant cluster (we'll have some fun with this later on by testing resilience against node failure)
    - Prometheus for scraping metrics
    - Grafana for metrics dashboarding
2. Ingestion
    - Utilizes [Qdrant dbpedia dataset with 1563 openai embeddings](https://huggingface.co/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-1536-1M)
    - Based on batching using upsert (as upload_collection does not support named vectors)
    - Parallelism to speed up ingestion
3. Search leveraging several strategies
    - Deprecated search function (as a performance baseline)
    - Sparse vector search
    - Dense vector with Binary Quantization
    - Hybrid search using Reciprocal Rank Fusion with Dense vector + Sparse vector
    - Hybrid search using Reciprocal Rank Fusion with Dense vector + Sparse vector and Binary Quantization
    - While measuring the latency of these queries

## üß† What is Qdrant?
Qdrant is a powerful, open-source vector database designed for scalable, high-performance similarity search and retrieval. It enables you to store, index, and search large collections of vector embeddings‚Äîmaking it ideal for applications like semantic search, recommendation systems, and AI-powered information retrieval. Qdrant supports advanced features such as distributed clustering, sharding, replication, and hybrid search, making it a robust choice for both research and production environments.

## üîç Background: Dense, Sparse, RRF, and BQ
**Dense Vectors:**
Dense vectors are high-dimensional, continuous representations of data (such as text or images) generated by neural network models like OpenAI's embedding models. They capture semantic meaning, allowing for similarity search based on the overall context and meaning of the input.

**Sparse Vectors:**
Sparse vectors are typically high-dimensional but contain mostly zeros, with only a few non-zero values. In information retrieval, sparse vectors often represent keyword-based features or term frequencies, capturing lexical (exact word) matches. Models like SPLADE generate such vectors for effective keyword-based retrieval.

**Reciprocal Rank Fusion (RRF):**
RRF is a technique for combining the results of multiple ranking methods (e.g., dense and sparse searches). By fusing the rankings, RRF leverages the strengths of both semantic (dense) and lexical (sparse) retrieval, often resulting in more relevant and robust search results.

**Binary Quantization (BQ):**
Binary Quantization is a method for compressing dense vectors into binary representations. This reduces memory usage and speeds up similarity computations, enabling faster search at scale. While there may be a slight trade-off in accuracy, BQ is highly effective for large-scale, high-throughput vector search scenarios.

## üõ†Ô∏è Prerequisites
This project assumes you have access to:
1. [Docker Desktop](https://docker.com) to run the containers
2. [OpenAI account](https://platform.openai.com/signup) to create embeddings for the queries
3. [uv](https://github.com/astral-sh/uv) ‚Äì a fast Python package installer and resolver, recommended for managing dependencies and running Python scripts efficiently

## üéí Preparations
### Setting up the 3 node Qdrant cluster
[docker-compose.yaml](./docker-compose.yaml) is being used to set up Qdrant and the observability stack.
```bash
docker compose up -d
```

Once all services have been started, the following user endpoints are avilable:
- [Qdrant dashboard](http://localhost:6333/dashboard#/welcome)
- [Grafana dashboard](http://localhost:3000/dashboard/db/qdrant-dashboard)
- [Prometheus dashboard](http://localhost:9090/)

### Configure settings
[.env](./.env) is being used to configure the:
- Qdrant endpoint
- OpenAI API Key
- Resource settings for ingestion `batch` and `parrelism`
- Collection settings for `sharding` and `replication factor`

Just copy [.env.example](./.env.example) to [.env](./.env) and asjust the settings as such:
- OPENAI_API_KEY: Set this to your OpenAI API key
- QDRANT_HOST: Set to the Qdrant endpoint (default: http://localhost:6333)

The following settings control resource consumption for ingestion:
- BATCH_SIZE: Defines how many points will be ingested in one go (default:100)
- MAX_WORKERS: Defines the amount of threads to use for parallel ingestion, depends heavily on your machine (default: 8)

Finally, the following settings control the behaviour of the collection:
- REPLICATION_FACTOR: Defines how many replicas of data will be stored across the cluster (default: 2)
- SHARD_NUMBER: Defines sharding across the cluster for this collection (default: 3)

### ‚öôÔ∏è Setting up your environment
It is recommended to use a virtual environment to manage Python dependencies. With [uv](https://github.com/astral-sh/uv), you can quickly set up and install everything you need:

```bash
# Create a new virtual environment (replace .venv with your preferred name)
uv venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install dependencies using pyproject.toml
uv install
```

This ensures your Python environment is isolated and all dependencies are installed efficiently.

## Ingesting the data
The dbpedia dataset with 1 million data points is ingested through [dbpedia_ingest_points_parallel.py](./dbpedia_ingest_points_parallel.py).

Some notes on the script:

### Collection configuration
Qdrant provides extensive controls on how vectors behave within the collection. Let's go step by step:
```python
client.create_collection(
    collection_name="dbpedia_entities_openai3",
    vectors_config={
        "dense": models.VectorParams(
            size=1536,
            distance=models.Distance.COSINE,
            on_disk=True # We store the dense vector on disk
        ),
    },
    sparse_vectors_config={
        "sparse": models.SparseVectorParams()
    },
    quantization_config=models.BinaryQuantization(
            binary=models.BinaryQuantizationConfig(always_ram=True), # We store the BQ vector in RAM
    ),
    hnsw_config=models.HnswConfigDiff(
        m=0, # We disable HNSW graph construction, which will allow for faster uploads, and we'll turn it on later
    ),
    shard_number=os.getenv("SHARD_NUMBER"),
    replication_factor=os.getenv("REPLICATION_FACTOR")
)
```
- First of all a `named dense vector` is defined with 1563 dimensions. This vector is being stored on disk in order to save memory consumption on the nodes.
- Then a `named sparse vector` is defined. This vector will be used in conjunction with the [prithivida/Splade_PP_en_v1](https://huggingface.co/prithivida/Splade_PP_en_v1) embedding model.
- Then `Binary Quantization` is configured which is forced to be stored in memory. The advantage of BQ is that a compressed form of the dense vector is being stored. Later on with querying this can be used in a two-pass that uses oversampling and ranking the results using the dense vector. This improves performance.
- In combination, the `HNSW` graph indexation is turned off to improve ingestion performance. Which later on will be turned on.
- And lastly, `sharding` and `replication` is configured.

### Ingestion
As `upload_collection` does not support multiple named vectors, instead `upsert` with manual batching and parellism is being used:
```python
def process_and_upload(batch):
    points = []
    for item in batch:
        sparse_embedding = list(sparse_model.embed(item["text"]))[0]
        points.append(
            models.PointStruct(
                id=str(uuid.uuid4()),
                vector={
                    "dense": item["text-embedding-3-large-1536-embedding"],
                    "sparse": {"indices": sparse_embedding.indices, "values": sparse_embedding.values}
                },
                payload={k: v for k, v in item.items() if k != "text-embedding-3-large-1536-embedding"}
            )
        )
    client.upsert(
        collection_name="dbpedia_entities_openai3",
        points=points
    )
```
- A list of `PointStruct` is being crafted based on the batch size.
- In here, both the `dense vector` and the `sparse vector` are being stored.
- In combination with a UUID as `id` and the payload from the dbpedia file, the data is stored in Qdrant.

### Enable graph construction
After ingestion the graph indexation is enabled again:
```python
client.update_collection(
    collection_name="dbpedia_entities_openai3",
    hnsw_config=models.HnswConfigDiff(
        m=16, # We enable HNSW graph construction
    )
)
```

### Ensure graph indexation is completed
Finally we wait until the indexation is finished, before we start querying:
```python
# Wait for indexing to complete
while True:
    status = client.get_collection("dbpedia_entities_openai3").status
    print(f"Indexing status: {status}")
    if status == "green":
        print("Indexing complete.")
        break
    time.sleep(5)
```

### Run the script
The script can be started using:
```bash
python dbpedia_ingest_points_parallel.py
```
Once started, watch the progress bar filling up and your CPU making over-hours.

Upon finalization, you can check the result by browsing to the [Qdrant dashboard](http://localhost:6333/dashboard#/welcome) and clicking 'Collections'.

## Quering
Qdrant has amazing hybrid search features that we'll use in this step. The following query mechanisms will be used while measuring latency:
- search (deprecated): Dense vector with Binary Quantization
- query_points: Dense vector
- query_points: Dense vector with Binary Quantization
- query_points: Reciprocal Rank Fusion with Dense vector + Sparse vector
- query_points: Reciprocal Rank Fusion with Dense vector + Sparse vector and Binary Quantization

Some notes on the script:

1. Search (Deprecated): Dense Vector with Binary Quantization
Method: `client.search`
Vector: Dense (OpenAI embedding)
Quantization: Binary Quantization enabled (compressed, in-memory representation for speed)
Params: `rescore=True` (rescoring with original vectors), `oversampling=3.0` (fetch more candidates for better recall)
Purpose: Baseline performance using the older search API with quantization for speed.

2. Query Points: Dense Vector
Method: `client.query_points`
Vector: Dense (OpenAI embedding)
Quantization: None
Params: Standard dense vector search, no quantization or fusion.
Purpose: Standard dense vector search, used as a baseline for comparison.

3. Query Points: Sparse Vector
Method: `client.query_points`
Vector: Sparse (Splade embedding)
Quantization: None
Params: Uses only the sparse vector for search.
Purpose: Tests the effectiveness of sparse retrieval (lexical/keyword-based) using a transformer-based sparse model.

4. Query Points: Dense Vector with Binary Quantization
Method: `client.query_points`
Vector: Dense (OpenAI embedding)
Quantization: Binary Quantization enabled (same as in the deprecated search)
Params: `rescore=True`, `oversampling=3.0`
Purpose: Modern API for dense search with quantization, for faster retrieval with some accuracy tradeoff.

5. Query Points: Reciprocal Rank Fusion (RRF) with Dense + Sparse
Method: `client.query_points`
Vectors: Both Dense and Sparse (prefetches both)
Fusion: RRF (Reciprocal Rank Fusion) combines rankings from both dense and sparse searches.
Params: Each vector fetches top 20 candidates, final results fused and top 5 returned.
Purpose: Hybrid search, leveraging both semantic (dense) and lexical (sparse) signals for improved relevance.

6. Query Points: RRF with Dense + Sparse and Binary Quantization
Method: `client.query_points`
Vectors: Both Dense and Sparse (prefetches both)
Fusion: RRF
Quantization: Binary Quantization enabled
Params: Combines hybrid search with quantization for speed.
Purpose: Fast, hybrid search that balances speed (via quantization) and relevance (via fusion of dense and sparse).

7. General Notes
- Timing: Each query measures and prints the time taken for performance benchmarking.
- Result Display: For each query, the top 5 results are printed with their titles and scores.
- Use Cases: The script is designed to compare different retrieval strategies in Qdrant, especially for large-scale (1M+ points) vector search scenarios.
- Hybrid Search: The RRF-based queries demonstrate Qdrant's ability to combine multiple retrieval signals, which is state-of-the-art for many search applications.

### Run the script
The script can be started using:
```bash
python dbpedia_search.py
```
Once started, watch the results come in with single-digit millisecond performance üéâ.

## üß™ Playing with Replication, Sharding, and Node Failures
A fun and insightful experiment you can do with this setup is to test Qdrant's resilience using replication factor (RF) and sharding. By setting a replication factor greater than 1 and enabling sharding in your .env file, your data will be distributed and redundantly stored across all three Qdrant nodes. This means you can simulate real-world failures and see how the system keeps running! üöÄ

### Steps to Try:

1. **Set up Replication and Sharding**  
   In your `.env` file, set:
   ```env
   REPLICATION_FACTOR=2
   SHARD_NUMBER=6
   ```
   (Or adjust as desired for your experiment.)

2. **Start the Cluster**  
   ```bash
   docker compose up -d
   ```

3. **Ingest Data**  
   ```bash
   python dbpedia_ingest_points_parallel.py
   ```

4. **Run queries**
   Run the queries and observe the results:
   ```bash
   python dbpedia_search.py
   ```

5. **Simulate Node Failure**  
   While ingestion or querying is running, stop one of the Qdrant nodes:
   ```bash
   docker compose stop qdrant-node-2
   ```
   (Or try stopping `qdrant-node-3`.)

6. **Observe the System**  
   - The cluster should remain operational, serving queries and ingesting data without interruption.  
   - Check the [Qdrant dashboard](http://localhost:6333/dashboard#/welcome) and [Grafana](http://localhost:3000/dashboard/db/qdrant-dashboard) to monitor cluster health and performance. üìä

7. **Run queries**
   Run the queries and observe the results, even with one node down!
   ```bash
   python dbpedia_search.py
   ```

8. **Restart the Node**  
   ```bash
   docker compose start qdrant-node-1
   ```
   The node will rejoin the cluster and synchronize automatically in case of any changes.

This hands-on approach demonstrates Qdrant's high availability and fault tolerance‚Äîperfect for understanding how distributed vector databases handle real-world failures! üõ°Ô∏è